{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-23T14:18:06.898998Z","iopub.execute_input":"2024-05-23T14:18:06.899253Z","iopub.status.idle":"2024-05-23T14:18:07.700225Z","shell.execute_reply.started":"2024-05-23T14:18:06.899229Z","shell.execute_reply":"2024-05-23T14:18:07.699246Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 --use-deprecated=legacy-resolver","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:18:07.701838Z","iopub.execute_input":"2024-05-23T14:18:07.702204Z","iopub.status.idle":"2024-05-23T14:18:35.906687Z","shell.execute_reply.started":"2024-05-23T14:18:07.702177Z","shell.execute_reply":"2024-05-23T14:18:35.905565Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you'll have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:18:35.908053Z","iopub.execute_input":"2024-05-23T14:18:35.908346Z","iopub.status.idle":"2024-05-23T14:18:55.271744Z","shell.execute_reply.started":"2024-05-23T14:18:35.908316Z","shell.execute_reply":"2024-05-23T14:18:55.270916Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-23 14:18:43.367710: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-23 14:18:43.367814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-23 14:18:43.503040: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:18:55.274023Z","iopub.execute_input":"2024-05-23T14:18:55.274314Z","iopub.status.idle":"2024-05-23T14:18:55.287845Z","shell.execute_reply.started":"2024-05-23T14:18:55.274289Z","shell.execute_reply":"2024-05-23T14:18:55.287037Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"ibm-granite/granite-8b-code-instruct\"\n\n# The instruction dataset to use\ndataset_name = \"Aremstrom/Spider_SQL\"\n\n# Fine-tuned model name\nnew_model = \"Granite-finetune_SQL\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 2\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:18:55.289359Z","iopub.execute_input":"2024-05-23T14:18:55.289658Z","iopub.status.idle":"2024-05-23T14:18:55.316381Z","shell.execute_reply.started":"2024-05-23T14:18:55.289634Z","shell.execute_reply":"2024-05-23T14:18:55.315643Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install flash_attn","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:18:55.317637Z","iopub.execute_input":"2024-05-23T14:18:55.318350Z","iopub.status.idle":"2024-05-23T14:19:20.734317Z","shell.execute_reply.started":"2024-05-23T14:18:55.318324Z","shell.execute_reply":"2024-05-23T14:19:20.733244Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting flash_attn\n  Downloading flash_attn-2.5.8.tar.gz (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash_attn) (2.1.2)\nCollecting einops (from flash_attn)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash_attn) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash_attn) (1.11.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->flash_attn) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash_attn) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash_attn) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash_attn\n  Building wheel for flash_attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash_attn: filename=flash_attn-2.5.8-cp310-cp310-linux_x86_64.whl size=120607435 sha256=b962f0eb38a2e54a3ece20b4b43f59f5a638ce53fe6e269992c58b119425d1f0\n  Stored in directory: /root/.cache/pip/wheels/9b/5b/2b/dea8af4e954161c49ef1941938afcd91bb93689371ed12a226\nSuccessfully built flash_attn\nInstalling collected packages: einops, flash_attn\nSuccessfully installed einops-0.8.0 flash_attn-2.5.8\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load dataset (you can process it here)\ntrain_ds = load_dataset(dataset_name, split='train[:80%]')\ntest_ds  = load_dataset(dataset_name, split='train[-20%:-10%]')\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:19:20.735749Z","iopub.execute_input":"2024-05-23T14:19:20.736057Z","iopub.status.idle":"2024-05-23T15:24:31.425189Z","shell.execute_reply.started":"2024-05-23T14:19:20.736026Z","shell.execute_reply":"2024-05-23T15:24:31.424370Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/273 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"839f24b47d4845e5a914c73b609c1dff"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 249k/249k [00:00<00:00, 496kB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d0c7b54ac77404a9d86288fcbfafbdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb49b5218134064a8433d0b04c706b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/46.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eec04f1ca734b5fbf8719836bd3ac10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eec400c55b0b45469f9f9d9c7cedca76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee90bf53555403a90a9f840545e8f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5924db09ee8a4e3daec25e2452386cee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba46cdb65647421dba00fe3e4a785289"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7366d8bb73fe4f5487301d6c05655cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ea5643717254e8b89361c6fd753e27b"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at ibm-granite/granite-8b-code-instruct were not used when initializing LlamaForCausalLM: ['model.layers.10.mlp.up_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.35.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.34.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.33.mlp.gate_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.35.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.32.mlp.down_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.33.self_attn.o_proj.bias', 'model.layers.33.mlp.up_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.35.self_attn.k_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.33.mlp.down_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.32.mlp.gate_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.34.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.32.self_attn.o_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.34.mlp.gate_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.32.mlp.up_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.35.mlp.down_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.11.self_attn.v_proj.bias']\n- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of LlamaForCausalLM were not initialized from the model checkpoint at ibm-granite/granite-8b-code-instruct and are newly initialized: ['model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'lm_head.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7360aa935ba745b19cd3c6c28310bf04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/4.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ce81290ec8c4d9d933e4d22aaf82ea4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42c6a4de3c82423bad8bb2f2d7498daa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e262f62a9624fbfbb1c7cf90b2b2c38"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d9d7b6c2e8846e98c22dfe5168dc813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0611bb16a1da4203a0a99ce3357318b0"}},"metadata":{}},{"name":"stderr","text":"You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [800/800 1:01:27, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>16.969600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>11.915300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>6.948000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.087800</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>2.136600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.194700</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.222300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.008400</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.025900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.696400</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.742900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.517300</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.654000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.518300</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.629200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.452100</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.583800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.460400</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.611300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.429600</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.589100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.413300</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.545000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.438700</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.539800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.412200</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.560400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.437000</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.554900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.441100</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.539600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.416000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=800, training_loss=1.802848916053772, metrics={'train_runtime': 3697.3769, 'train_samples_per_second': 0.865, 'train_steps_per_second': 0.216, 'total_flos': 1.04653310926848e+16, 'train_loss': 1.802848916053772, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"eval_results = trainer.evaluate()\neval_results","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:24:31.426318Z","iopub.execute_input":"2024-05-23T15:24:31.426606Z","iopub.status.idle":"2024-05-23T15:25:59.695957Z","shell.execute_reply.started":"2024-05-23T15:24:31.426579Z","shell.execute_reply":"2024-05-23T15:25:59.695158Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 01:24]\n    </div>\n    "},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.49619269371032715,\n 'eval_runtime': 88.2581,\n 'eval_samples_per_second': 2.266,\n 'eval_steps_per_second': 0.283,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"code","source":"# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:25:59.697031Z","iopub.execute_input":"2024-05-23T15:25:59.697328Z","iopub.status.idle":"2024-05-23T15:25:59.892359Z","shell.execute_reply.started":"2024-05-23T15:25:59.697302Z","shell.execute_reply":"2024-05-23T15:25:59.891389Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir results/runs","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:25:59.896108Z","iopub.execute_input":"2024-05-23T15:25:59.896432Z","iopub.status.idle":"2024-05-23T15:26:07.949405Z","shell.execute_reply.started":"2024-05-23T15:25:59.896405Z","shell.execute_reply":"2024-05-23T15:26:07.948449Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"Select Unique Value\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:26:07.950592Z","iopub.execute_input":"2024-05-23T15:26:07.950972Z","iopub.status.idle":"2024-05-23T15:27:37.807858Z","shell.execute_reply.started":"2024-05-23T15:26:07.950937Z","shell.execute_reply":"2024-05-23T15:27:37.806828Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] Select Unique Value [/INST]]}]\n    Answer the following question. Given that the first match was against the United States and the second match was against the United States, what was the score of the third match?\nThe third match was a rematch of the first and second matches. The score of the first match was 3-0, and the score of the second match was 2-1. The score of the third match was 3-0.\nThe score of the third match was 3-0.\nThe score of the third match was 3-0.\n\n3-0 (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(result)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:27:37.809365Z","iopub.execute_input":"2024-05-23T15:27:37.809853Z","iopub.status.idle":"2024-05-23T15:27:37.815201Z","shell.execute_reply.started":"2024-05-23T15:27:37.809810Z","shell.execute_reply":"2024-05-23T15:27:37.814380Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[{'generated_text': '<s>[INST] Select Unique Value [/INST]]}]\\n    Answer the following question. Given that the first match was against the United States and the second match was against the United States, what was the score of the third match?\\nThe third match was a rematch of the first and second matches. The score of the first match was 3-0, and the score of the second match was 2-1. The score of the third match was 3-0.\\nThe score of the third match was 3-0.\\nThe score of the third match was 3-0.\\n\\n3-0 (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0) (3-0)'}]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Empty VRAM\ndel model\ndel pipe\ndel trainer\nimport gc\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:27:37.816718Z","iopub.execute_input":"2024-05-23T15:27:37.817034Z","iopub.status.idle":"2024-05-23T15:27:38.497467Z","shell.execute_reply.started":"2024-05-23T15:27:37.817006Z","shell.execute_reply":"2024-05-23T15:27:38.496506Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:27:38.498703Z","iopub.execute_input":"2024-05-23T15:27:38.499006Z","iopub.status.idle":"2024-05-23T15:29:38.838209Z","shell.execute_reply.started":"2024-05-23T15:27:38.498979Z","shell.execute_reply":"2024-05-23T15:29:38.837357Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11cade292476449faefe0f8db761880a"}},"metadata":{}}]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:29:38.839396Z","iopub.execute_input":"2024-05-23T15:29:38.839724Z","iopub.status.idle":"2024-05-23T15:29:38.844120Z","shell.execute_reply.started":"2024-05-23T15:29:38.839690Z","shell.execute_reply":"2024-05-23T15:29:38.843079Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:38:36.502019Z","iopub.execute_input":"2024-05-23T16:38:36.503045Z","iopub.status.idle":"2024-05-23T16:38:36.528025Z","shell.execute_reply.started":"2024-05-23T16:38:36.503000Z","shell.execute_reply":"2024-05-23T16:38:36.527045Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"789ad40b48604a2b9d6e319661145cae"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"Aremstrom/Granite_FineTunned_Text_2_SQL\", check_pr=True)\n\ntokenizer.push_to_hub(\"Aremstrom/Granite_FineTunned_Text_2_SQL\",check_pr=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:40:03.713192Z","iopub.execute_input":"2024-05-23T16:40:03.713575Z","iopub.status.idle":"2024-05-23T16:46:28.695808Z","shell.execute_reply.started":"2024-05-23T16:40:03.713543Z","shell.execute_reply":"2024-05-23T16:46:28.694822Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca1848ea1e824e6ab7bc2c7ebf927da5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0275f246577e4ad4a2d41f6dbe1ee4a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/6.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76721efbc5a948808ca84c9eaac403cf"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Aremstrom/Granite_FineTunned_Text_2_SQL/commit/1d4456d57ad58d9817897bb868837bbc3e79b408', commit_message='Upload tokenizer', commit_description='', oid='1d4456d57ad58d9817897bb868837bbc3e79b408', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}